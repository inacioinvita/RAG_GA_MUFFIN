!pip install langchain-openai
!pip install pypdf
!pip install tiktoken
!pip install 'scikit-learn>=1.3.1'
!pip install --upgrade 'protobuf<4.21,>=3.20.3'
!pip install 'nbconvert>=7.14.2'
!pip install trulens_eval openai langchain chromadb


import os
import openai
import sys

from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from IPython.display import clear_output

from trulens_eval import TruChain # May need to be run twice or three times

from trulens_eval import Feedback
from trulens_eval import Tru
from trulens_eval.feedback import Groundedness
from langchain.document_loaders import PyPDFLoader

# clear_output()

from google.colab import drive
drive.mount('/content/drive')

sys.path.append('../..')
os.environ["OPENAI_API_KEY"] = "<your_api>"
openai.api_key  = os.environ['OPENAI_API_KEY']

loader = PyPDFLoader('/content/drive/MyDrive/MOOC-2 HumanSensing.pdf')
pages = loader.load()

chunk_size=500

text_splitter = CharacterTextSplitter(
    separator=" ",
    chunk_size=chunk_size,
    chunk_overlap=30,
    length_function=len
)

splits = text_splitter.split_documents(pages)
print(len(pages))
print(len(splits))


embedding = OpenAIEmbeddings()


persist_directory = '/content/docs/chroma2'

!rm -rf ./docs/chroma

vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)

# vectordb.persist()

llm_name = "gpt-3.5-turbo"
# llm_name = "gpt-4"
llm = ChatOpenAI(model_name=llm_name, temperature=0)

template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say "I don't know the answer based on MOOC 2: Sensing the Human", don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "I hope this helps with your 2024_CA4015 assignment!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""

QA_CHAIN_PROMPT = PromptTemplate.from_template(template)


qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

# **Evaluation**

from trulens_eval.feedback.provider import OpenAI
import numpy as np

openai = OpenAI()

from trulens_eval.app import App
context = App.select_context(qa_chain)



grounded = Groundedness(groundedness_provider=OpenAI())
f_groundedness = (
    Feedback(grounded.groundedness_measure_with_cot_reasons)
    .on(context.collect())
    .on_output()
    .aggregate(grounded.grounded_statements_aggregator)
)

f_qa_relevance = Feedback(openai.relevance).on_input_output()

f_context_relevance = (
    Feedback(openai.qs_relevance)
    .on_input()
    .on(context)
    .aggregate(np.mean)
)

tru = Tru()
tru.run_dashboard() # open a local streamlit app to explore

# tru.stop_dashboard() # stop if needed

questions = [
    "How do you aggregate health sensor data?",
    "What are the sleep stages?",
    "What information does 23andMe provide?",
    "What is the brain responsible for?",
    "What's EEG?"
]

templates = ["""Use the following pieces of context to answer the question at the end. If you don't know the answer, just say "I don't know the answer based on MOOC 2: Sensing the Human", don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "I hope this helps with your 2024_CA4015 assignment!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:""", """Informed by the context from 'MOOC 2: Sensing the Human,' please provide an answer to the question below. Your answer must directly draw upon the given information, ensuring accuracy and groundedness. If the answer is not clear from the context, respond with 'The information provided does not sufficiently answer the question based on MOOC 2: Sensing the Human.' Aim for clarity and brevity, using up to three sentences. Conclude your response with 'I hope this clarification enhances your grasp of MOOC 2: Sensing the Human.'
{context}
Question: {question}
Insightful Answer:""", """Given the specific context extracted from 'MOOC 2: Sensing the Human,' answer the following question succinctly. Ensure your response is tightly aligned with the context, highlighting the most relevant information. If the context does not contain an answer, politely indicate so by stating, 'Unable to determine an answer based on MOOC 2: Sensing the Human.' Your response should not exceed three sentences and should end with 'I hope this aids in your understanding of MOOC 2: Sensing the Human.'
{context}
Question: {question}
Relevant Response:""", """Utilize the context provided from 'MOOC 2: Sensing the Human' to accurately answer the query that follows. Your response should demonstrate a thorough consideration of the context, pinpointing details directly relevant to the question. If the context lacks sufficient details for an answer, simply state, 'The provided context does not offer enough information for a conclusive answer on MOOC 2: Sensing the Human.' Limit your answer to three sentences, and finish with 'I trust this response is useful for your exploration of MOOC 2: Sensing the Human.'
{context}
Question: {question}
Informed Response:""", """Drawing on the context from 'MOOC 2: Sensing the Human,' please formulate a precise answer to the subsequent question. Ensure that your response is grounded in the provided context, accurately reflecting the content. Should the context fall short of providing a clear answer, kindly note, 'The context does not enable a precise answer regarding MOOC 2: Sensing the Human.' Strive for precision and conciseness within a three-sentence limit, concluding with 'I hope this contributes to your understanding of MOOC 2: Sensing the Human.'
{context}
Question: {question}
Exact Answer:""", """Consider the information given in the context carefully to construct a logical chain of thought that leads to an accurate answer. If the answer isn't directly available from 'MOOC 2: Sensing the Human', explicitly state 'I don't know the answer based on MOOC 2: Sensing the Human' instead of guessing. Summarize the reasoning in three sentences or fewer, concluding with a direct answer or acknowledgment of the absence of an answer. Always end with 'I hope this helps with your 2024_CA4015 assignment!' Here's the context:
{context}
Given this, how would you answer the following question: {question}
Your logical reasoning and helpful answer:""",
             """Without prior knowledge, use the context provided to derive an answer. If the information is insufficient or the answer is not contained within 'MOOC 2: Sensing the Human', clearly state 'I don't know the answer based on MOOC 2: Sensing the Human'. Provide a concise answer in no more than three sentences, ensuring it's directly relevant to the question asked. Conclude with the phrase 'I hope this helps with your 2024_CA4015 assignment!' Context for consideration:
{context}
Question at hand: {question}
Your insightful and helpful answer:""",
             """Analyze the context provided critically, identifying key points relevant to the question. If the answer cannot be confidently determined from the information in 'MOOC 2: Sensing the Human', state 'I don't know the answer based on MOOC 2: Sensing the Human' rather than speculating. Limit your response to three sentences, focusing on clarity and accuracy. Finish with 'I hope this helps with your 2024_CA4015 assignment!' Review the following context:
{context}
With this in mind, what's your answer to the question: {question}
Your critically thought-out and helpful answer:""",
             """Explore the context given below to find elements that contribute to a grounded answer to the question. If the answer is not evident from 'MOOC 2: Sensing the Human', please specify 'I don't know the answer based on MOOC 2: Sensing the Human' and avoid conjecture. Answer succinctly within three sentences, ensuring your reply is informed by the context. Always close with 'I hope this helps with your 2024_CA4015 assignment!' Here's your context:
{context}
Contemplating this, how would you respond to the question: {question}
Your informed and concise answer:""",
             """Putting yourself in the shoes of someone studying 'MOOC 2: Sensing the Human', use the context to provide a thoughtful answer. If the context does not yield a clear answer, say 'I don't know the answer based on MOOC 2: Sensing the Human', ensuring not to fabricate a response. Keep your answer to a maximum of three sentences, directly addressing the query. End with 'I hope this helps with your 2024_CA4015 assignment!' Context to consider:
{context}
In light of that, what would your answer be to: {question}
Your empathetic and helpful answer:"""]

chunk_sizes=[100, 200, 400, 500, 700]

llm_names = ["gpt-3.5-turbo", "gpt-4"]


for l_index, llm_name in enumerate (llm_names, start=1):
  llm = ChatOpenAI(model_name=llm_name, temperature=0)

  for c_index, chunk_size in enumerate(chunk_sizes, start=1):
    text_splitter = CharacterTextSplitter(
      separator=" ",
      chunk_size=chunk_size,
      chunk_overlap=30,
      length_function=len
    )

    splits = text_splitter.split_documents(pages)

    persist_directory = '/content/docs/chroma2'

    !rm -rf ./docs/chroma

    vectordb = Chroma.from_documents(
        documents=splits,
        embedding=embedding,
        persist_directory=persist_directory
    )

    for q_index, question in enumerate(questions, start=1):
        for t_index, template_text in enumerate(templates, start=1):

            QA_CHAIN_PROMPT = PromptTemplate.from_template(template_text)


            qa_chain = RetrievalQA.from_chain_type(
                llm,
                retriever=vectordb.as_retriever(),
                return_source_documents=True,
                chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
            )


            # Dynamically generate app_id based on question and template indices
            # A: separator="/n", chunk_size=150, chunk_overlap=30,
            # B: separator=" ", chunk_size=150, chunk_overlap=30,
            # C: separator=" ", chunk_size=250, chunk_overlap=30,
            # D: separator=" ", chunk_size=100, chunk_overlap=30,
            # E: separator=" ", chunk_size=350, chunk_overlap=30,
            # AA: separator="/n", chunk_size=150, chunk_overlap=30, Second group of templates
            # BA: separator=" ", chunk_size=150, chunk_overlap=30,
            # CA: separator=" ", chunk_size=250, chunk_overlap=30,
            # DA: separator=" ", chunk_size=100, chunk_overlap=30,
            # EA: separator=" ", chunk_size=350, chunk_overlap=30,
            # EB: separator=" ", chunk_size=350, chunk_overlap=30, map_reduce

            app_id = f"{llm_name}_{chunk_size}_q{q_index}_t{t_index}_Chain1_ChatApplication"

            tru_recorder = TruChain(
                qa_chain,
                app_id=app_id,
                feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness]
            )

            # tru_recorder.with_record(qa_chain.invoke, {"query": question})
            response, tru_record = tru_recorder.with_record(qa_chain.invoke, question)
            # After the loop, results can be collected from the TruLens dashboard


import pandas as pd

# Excel file exported from Tru Dashboard with the results

file_path = '/content/drive/MyDrive/export (5).xlsx'
df = pd.read_excel(file_path)

# @title relevance

from matplotlib import pyplot as plt
df['relevance'].plot(kind='hist', bins=20, title='relevance')
plt.gca().spines[['top', 'right',]].set_visible(False)

# Final scores were analysed with a focus on the combination of chunk_size and template

def extract_chunk_size(app_id):
    try:
        return int(app_id.split('_')[1])
    except ValueError:
        return None

def extract_t_index(app_id):
    try:
        start = app_id.find('_t') + 2
        end = start
        while end < len(app_id) and app_id[end].isdigit():
            end += 1
        return int(app_id[start:end])
    except ValueError:
        return None

df['chunk_size'] = df['App ID'].apply(extract_chunk_size)
df['t_index'] = df['App ID'].apply(extract_t_index)

df_filtered = df.dropna(subset=['chunk_size', 't_index'])

df_filtered['aggregate_score'] = (df_filtered['relevance'] + df_filtered['qs_relevance'] + df_filtered['groundedness_measure_with_cot_reasons']) / 3

grouped_df_filtered = df_filtered.groupby(['chunk_size', 't_index'])['aggregate_score'].mean().reset_index()

top_combinations_filtered = grouped_df_filtered.sort_values(by='aggregate_score', ascending=False).head(10)

top_combinations_filtered
